{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "#\n",
    "# Copyright 2024\n",
    "# Heinrich Heine University Dusseldorf,\n",
    "# Faculty of Mathematics and Natural Sciences,\n",
    "# Computer Science Department\n",
    "#\n",
    "# Authors:\n",
    "# Renato Vukovic (renato.vukovic@hhu.de)\n",
    "#\n",
    "# This code was generated with the help of AI writing assistants\n",
    "# including GitHub Copilot, ChatGPT, Bing Chat.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the Ontology Relation Prediction Data generate the instruction/prompt inputs for fine-tuning and the relational triplets as outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_description': 'You are an expert in ontology construction from a set of domain (general topic), slot (information type about entities in a domain) or value (concrete instances of information in slots) candidate terms in task-oriented dialogue.', 'dialogue': 'Here is the dialogue:', 'term_list': 'Here is the list of terms:', 'relations_so_far': '', 'output_instruction': \"Predict relations in the dialogue between the given terms, that can be domains, slots or values. Predict the 'has slot' relation between domains and slots in the form [domainname, has slot, slotname]. Predict the 'has value' relation between slots and values in the form [slotname, has value, valuename]. Predict the 'has domain' relation between values and domains in the form [valuename, has domain, domainname]. Predict the 'refers to same concept as' relation between terms from the same category with the same meaning [term, refers to same concept as, term]. Make sure to put brackets around each relational triplet!\"}\n"
     ]
    }
   ],
   "source": [
    "#load the prompt dict\n",
    "with Path(\"../experiments/prompts/no_memory/zero_shot_no_memory_reframed_prompt_dict.json\").open(\"r\") as f:\n",
    "\tprompt_dict = json.load(f)\n",
    "\n",
    "print(prompt_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First MultiWOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"./multiwoz21_dialogue_term_dict.json\").open(\"r\") as f:\n",
    "\tdata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'terms', 'relational triplets'])\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][\"multiwoz21-train-0\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = prompt_dict[\"task_description\"]\n",
    "dialogue_input = prompt_dict[\"dialogue\"]\n",
    "term_input = prompt_dict[\"term_list\"]\n",
    "output_instruction = prompt_dict[\"output_instruction\"]\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8438/8438 [00:00<00:00, 185597.24it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 169754.90it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 167450.65it/s]\n"
     ]
    }
   ],
   "source": [
    "sft_data_dict = {}\n",
    "#go through the splits and create the dataset with the instruction consisting of the prompt, the dialogue and the term list\n",
    "for split in data.keys():\n",
    "    sft_data_dict[split] = {\"instruction\": [], \"output\": [], \"dialogue_id\": []}\n",
    "    for dial_id, dialogue in tqdm(data[split].items()):\n",
    "        text = dialogue[\"text\"]\n",
    "        term_list = dialogue[\"terms\"]\n",
    "        relation_triplets = dialogue[\"relational triplets\"]\n",
    "        instruction_text = \"\"\n",
    "        instruction_text += dialogue_input + \"\\n\" + text + \"\\n\"\n",
    "        instruction_text += term_input + \"\\n\" + str(term_list)  + \"\\n\"\n",
    "        instruction_text += output_instruction + \"\\n\"\n",
    "\n",
    "        sft_data_dict[split][\"instruction\"].append(instruction_text)\n",
    "        sft_data_dict[split][\"output\"].append(relation_triplets)\n",
    "        sft_data_dict[split][\"dialogue_id\"].append(dial_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make datasets out of the different split dicts\n",
    "sft_datasets = {}\n",
    "for split in sft_data_dict.keys():\n",
    "    sft_datasets[split] = Dataset.from_dict(sft_data_dict[split])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 8438\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#turn the dataset into a Huggingface dataset\n",
    "sft_dataset = DatasetDict(sft_datasets)\n",
    "\n",
    "print(sft_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5e02781e2844cb814fc9d63e52a5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8438 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90118707fd84383964f37cc1095de0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840fcaa183f441f499b06d8814aa34c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save the dataset\n",
    "sft_dataset.save_to_disk(\"./multiwoz21_ontology_relation_sft_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 8438\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#test loading the dataset\n",
    "sft_dataset = load_from_disk(\"./multiwoz21_ontology_relation_sft_dataset\")\n",
    "\n",
    "print(sft_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"./sgd_dialogue_term_dict.json\").open(\"r\") as f:\n",
    "\tdata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16142/16142 [00:00<00:00, 326404.51it/s]\n",
      "100%|██████████| 2482/2482 [00:00<00:00, 250626.25it/s]\n",
      "100%|██████████| 4201/4201 [00:00<00:00, 278198.70it/s]\n"
     ]
    }
   ],
   "source": [
    "sft_data_dict = {}\n",
    "#go through the splits and create the dataset with the instruction consisting of the prompt, the dialogue and the term list\n",
    "for split in data.keys():\n",
    "    sft_data_dict[split] = {\"instruction\": [], \"output\": [], \"dialogue_id\": []}\n",
    "    for dial_id, dialogue in tqdm(data[split].items()):\n",
    "        text = dialogue[\"text\"]\n",
    "        term_list = dialogue[\"terms\"]\n",
    "        relation_triplets = dialogue[\"relational triplets\"]\n",
    "        instruction_text = \"\"\n",
    "        instruction_text += dialogue_input + \"\\n\" + text + \"\\n\"\n",
    "        instruction_text += term_input + \"\\n\" + str(term_list)  + \"\\n\"\n",
    "        instruction_text += output_instruction + \"\\n\"\n",
    "\n",
    "        sft_data_dict[split][\"instruction\"].append(instruction_text)\n",
    "        sft_data_dict[split][\"output\"].append(relation_triplets)\n",
    "        sft_data_dict[split][\"dialogue_id\"].append(dial_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make datasets out of the different split dicts\n",
    "sft_datasets = {}\n",
    "for split in sft_data_dict.keys():\n",
    "    sft_datasets[split] = Dataset.from_dict(sft_data_dict[split])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 16142\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 2482\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 4201\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#turn the dataset into a Huggingface dataset\n",
    "sft_dataset = DatasetDict(sft_datasets)\n",
    "\n",
    "print(sft_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbe141844e040ebb7651de0d661fa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/16142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a78feb918a8449aa44be44aa22ab599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3522d8e9f243acbb2f90f84b2d473b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save the dataset\n",
    "sft_dataset.save_to_disk(\"./sgd_ontology_relation_sft_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 16142\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 2482\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'output', 'dialogue_id'],\n",
      "        num_rows: 4201\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#test loading the dataset\n",
    "sft_dataset = load_from_disk(\"./sgd_ontology_relation_sft_dataset\")\n",
    "\n",
    "print(sft_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34866bf508015a88ad6061e6f60cc3937ab4ccd761b81ec6610a1b8b08b26369"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
