# coding=utf-8
#
# Copyright 2024
# Heinrich Heine University Dusseldorf,
# Faculty of Mathematics and Natural Sciences,
# Computer Science Department
#
# Authors:
# Renato Vukovic (renato.vukovic@hhu.de)
#
# This code was generated with the help of AI writing assistants
# including GitHub Copilot, ChatGPT, Bing Chat.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# # # # # # # # # # # # # # # # # # # # # # # # # # # # #

#define the LLM zeroshot prediction config with parameters

class LLM_prediction_config():

	model_name: str #name of the model
	dataset: str #name of the dataset
	splits: list[str] #list of splits to be used for the prediction
	max_model_length: int #maximum length of the model
	max_new_tokens: int #maximum number of new tokens to be generated by the model
	steps: int #number of steps to be used for the prediction prompt
	subset_size: int #size of the subset to be used for the prediction
	only_hasslot: bool #whether to only use the hasslot relation in the prompt
	only_hasvalue: bool #whether to only use the hasvalue relation in the prompt
	only_hasdomain: bool #whether to only use the hasdomain relation in the prompt
	only_equivalence: bool #whether to only use the equivalence relation in the prompt
	oneshot: bool #whether to use the oneshot prompt
	finetuned_model: bool #whether to use a finetuned model for the prediction
	constrain_generation: bool #whether to constrain the generation with custom constraints for the relation prediction
	predict_for_cot_decoding: bool #whether to predict for the CoT decoding, i.e. add the input ids and the top 2 logits for each token in each branch
	analyse_top_k_tokens: bool #whether to analyse the top k tokens in the prompt
	entropy_branching_threshold: float #threshold for the entropy branching in cot decoding
	instruction_prefix: str #prefix for the instruction in the prompt
	answer_prefix: str #prefix for the answer in the prompt
	seed: int #random seed for the prediction

	def __init__(self, model_name, dataset, splits, max_model_length=4096, max_new_tokens=1000, steps=1, subset_size=None, only_hasslot=False, only_hasvalue=False, only_hasdomain=False, only_equivalence=False, oneshot=False, exemplar_from_sgd=False, finetuned_model=False, constrain_generation=False, predict_for_cot_decoding=False, analyse_top_k_tokens=False, entropy_branching_threshold=0., instruction_prefix="prompt", answer_prefix="completion", seed=None):
		self.model_name = model_name
		self.dataset = dataset
		self.splits = splits
		self.max_model_length = max_model_length
		self.max_new_tokens = max_new_tokens
		self.steps = steps

		self.subset_size = subset_size

		self.only_hasslot = only_hasslot
		self.only_hasvalue = only_hasvalue
		self.only_hasdomain = only_hasdomain
		self.only_equivalence = only_equivalence

		self.oneshot = oneshot
		self.exemplar_from_sgd = exemplar_from_sgd

		self.finetuned_model = finetuned_model

		self.constrain_generation = constrain_generation

		self.predict_for_cot_decoding = predict_for_cot_decoding

		self.analyse_top_k_tokens = analyse_top_k_tokens

		self.entropy_branching_threshold = entropy_branching_threshold

		self.instruction_prefix = instruction_prefix
		self.answer_prefix = answer_prefix

		self.seed = seed

	#method for returning the config params as a dict for saving as json
	def to_dict(self):
		return vars(self)
		